{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":"<p>Welcome to Chisel \u2014 a lightweight and extensible Python library for preparing token-level annotation datasets for tasks like Named Entity Recognition (NER), Span Classification, and beyond.</p> <p>Chisel tries to make it easy for NLP practitioners to experiment with different models when doing their extraction tasks by standardising the preprocessing steps and validation - making it easy to swap out models with different data and preprocessing requirements without spending ages on writing ad-hoc code.</p> <p>This guide will walk you through setting up Chisel, understanding the core concepts, and running your first processing pipeline.</p>"},{"location":"#installation","title":"\ud83e\uddf0 Installation","text":"<pre><code>pip install -e .  # From source\n</code></pre> <p>Ensure you have a compatible Python version (3.8\u20133.11 recommended) and install dependencies:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"#core-concepts","title":"\ud83e\uddf1 Core Concepts","text":"<p>Chisel follows a modular pipeline architecture with pluggable components:</p> Component Role Parser Extracts raw entity spans from annotated documents Tokenizer Splits raw text into tokens Chunker Optionally breaks long documents into chunks to fit token windows SpanAligner Maps character spans to tokens Labeler Converts token-level alignment into BIO, BILOU, or binary tags ParseValidator Ensures consistency between text and character spans TokenValidator Ensures consistency between character spans, tokens and labels Formatters Turns the final processed data to your preferred format <p>Each component follows a defined Protocol, so you can swap in custom implementations as needed.</p>"},{"location":"#examples","title":"\ud83d\ude80 Examples","text":"<p>For examples as to how to use Chisel in pracice with common data annotations such as html tags, conll format or json, take a look at the notebooks in the example folder.</p>"},{"location":"#development-roadmap-in-no-particular-order","title":"\ud83d\udea7 Development roadmap (in no particular order)","text":"<ul> <li>Implement exporters for a broad range of data providers such as HuggingFaces Datasets, Pytorch Data, Spacy and DVC</li> <li>Implement parsers for common annotation tools such as Doccano, LabelStudio etc.</li> <li>Ensure compatibility with modern tokenizers such as BPE.</li> <li>Implement more sophisticated chunking methods for models with smaller token limits (like DistilBert 512)</li> <li>Design a principled and declarative way to build pipelines.</li> </ul> <p>\ud83d\udcd6 What's Next?</p> <ul> <li>Learn how each component works</li> <li>Check out real-world examples</li> <li>Explore reference docs</li> </ul>"},{"location":"components/aligners/","title":"\ud83e\udde9 SpanAligners","text":"<p>Aligners are responsible for mapping annotated character-based entity spans onto their corresponding token indices after tokenization. This step is crucial for generating label sequences in BIO, BILOU, or binary tagging formats.</p>"},{"location":"components/aligners/#responsibilities","title":"\u2705 Responsibilities","text":"<p>Take in a list of Token objects and a list of EntitySpan objects.</p> <p>Return a list of TokenEntitySpan objects, which attach each entity to the indices of the tokens that represent it.</p> <p>Ensure accurate alignment even when tokenization introduces splitting (e.g., subwords from BPE tokenizers).</p>"},{"location":"components/aligners/#interfaces","title":"\ud83d\udee0 Interfaces","text":"<p>TokenAligner Protocol <pre><code>class TokenAligner(Protocol):\n    def align(self, tokens: List[Token], entities: List[EntitySpan]) -&gt; List[TokenEntitySpan]:\n        ...\n</code></pre> This protocol defines the expected interface for all aligners.</p>"},{"location":"components/aligners/#implementations","title":"\ud83d\ude80 Implementations","text":""},{"location":"components/aligners/#huggingfacetokenaligner","title":"HuggingfaceTokenAligner","text":"<p>An alignment strategy designed for Huggingface-style tokenizers. It works by:</p> <ul> <li> <p>Matching token character spans with entity spans.</p> </li> <li> <p>Mapping each entity to the smallest contiguous list of token indices that fully cover its span.</p> </li> <li> <p>Optionally normalizing whitespace and punctuation to improve matching robustness.</p> </li> </ul>"},{"location":"components/aligners/#output","title":"Output","text":"<ul> <li>Each aligner returns a list of TokenEntitySpan objects:</li> </ul> <pre><code>TokenEntitySpan(\n    entity=EntitySpan(\n        text=\"Barack Obama\",\n        start=0,\n        end=12,\n        label=\"PER\"\n    ),\n    token_indices=[0, 1]\n)\n</code></pre>"},{"location":"components/aligners/#validation","title":"\ud83d\udd0e Validation","text":"<p>Aligners should be paired with validators (like ValidateLabelAlignment) to ensure that the token spans can accurately reconstruct the original entity text after tokenization. This helps catch tokenizer mismatches or annotation inconsistencies.</p>"},{"location":"components/aligners/#example-usage","title":"\ud83d\udcda Example Usage","text":"<pre><code>tokens = tokenizer.tokenize(\"Barack Obama visited the USA.\")\nentities = [EntitySpan(text=\"Barack Obama\", start=0, end=12, label=\"PER\")]\n\naligned = aligner.align(tokens, entities)\n\n# Produces token indices covering the span \"Barack Obama\"\n</code></pre>"},{"location":"components/chunkers/","title":"\ud83d\udd2a Chunkers","text":"<p>Chunkers control how long texts are broken into smaller segments (chunks) for token classification tasks.</p> <p>This is useful when: - A document is too long for the model\u2019s input length. - You want to process text in overlapping windows. - You need to preserve entity and token alignment across segments.</p>"},{"location":"components/chunkers/#chunker-protocols","title":"\ud83e\udde9 Chunker Protocols","text":"<p>Chisel supports two kinds of chunkers:</p>"},{"location":"components/chunkers/#1-tokenchunker","title":"1. <code>TokenChunker</code>","text":"<p>Used when tokenization is already performed before chunking.</p> <pre><code>class TokenChunker(Protocol):\n    def chunk(self, tokens: List[Token], entities: List[TokenEntitySpan]) -&gt; Tuple[List[List[Token]], List[List[TokenEntitySpan]]]:\n        ...\n</code></pre> <p>Returns a tuple with a list of lists of Token objects and a list of lists of TokenEntitySpan objects:</p> <ul> <li> <p>\"tokens\": the list of Token objects in the chunk.</p> </li> <li> <p>\"entities\": the list of EntitySpans that fall within the chunk.</p> </li> </ul>"},{"location":"components/chunkers/#2-textchunker","title":"2. TextChunker","text":"<p>Used to chunk text before tokenization.</p> <pre><code>class TextChunker(Protocol):\n    def chunk(self, text: str, entities: List[EntitySpan]) -&gt; Tuple[List[List[str]], List[List[EntitySpan]]]:\n        ...\n</code></pre> <p>This is useful for early preprocessing stages or non-token-based chunking.</p>"},{"location":"components/chunkers/#built-in-chunkers","title":"\ud83e\uddf1 Built-in Chunkers","text":""},{"location":"components/chunkers/#noopchunker","title":"NoOpChunker","text":"<p>Does not chunk. Returns the full document as a single chunk.</p> <pre><code>from chisel.chunkers import NoOpChunker\n\nchunker = NoOpChunker()\nchunks = chunker.chunk(tokens, entities)\n</code></pre>"},{"location":"components/chunkers/#fixedlengthchunker","title":"FixedLengthChunker","text":"<p>Splits tokens into non-overlapping fixed-size chunks.</p> <p><pre><code>from chisel.chunkers import FixedLengthChunker\n\nchunker = FixedLengthChunker(chunk_size=128)\nchunks = chunker.chunk(tokens, entities)\n</code></pre> Entities that do not fully fit within the chunk are excluded.</p>"},{"location":"components/chunkers/#notes-on-entity-alignment","title":"\u2699\ufe0f Notes on Entity Alignment","text":"<p>Chunkers are responsible for excluding entities that cross chunk boundaries.</p> <p>This ensures downstream labelers do not mislabel partial entities.</p>"},{"location":"components/chunkers/#custom-chunkers","title":"\ud83d\udee0\ufe0f Custom Chunkers","text":"<p>You can implement your own chunking strategy by subclassing or following the TokenChunker/TextChunker protocol.</p> <p>Example:</p> <pre><code>class SentenceChunker:\n    def chunk(self, tokens, entities):\n        # Group tokens into sentences using punctuation rules\n        ...\n</code></pre>"},{"location":"components/exporters/","title":"\ud83d\udce4 Exporters","text":""},{"location":"components/exporters/#note-only-protocol-implemented-will-implement-exporters-to-persist-data-to-huggingface-dvc-spacy-format-etc-in-future","title":"\u26a0\ufe0f NOTE: Only protocol implemented. Will implement exporters to persist data to huggingface, DVC, spacy format etc in future.","text":"<p>Exporters define how the final, processed data is saved, serialized, or made available to downstream tasks like model training or data inspection.</p> <p>Chisel comes with built-in exporters for common formats, and you can also implement your own.</p>"},{"location":"components/exporters/#exporter-interface","title":"\ud83e\udde9 Exporter Interface","text":"<pre><code>class Exporter(Protocol):\n    def export(self, data: List[Dict]) -&gt; None:\n        ...\n</code></pre> <ul> <li> <p>data: A list of processed data samples.</p> </li> <li> <p>Each sample is typically a dictionary with fields like input_ids, labels, tokens, etc.</p> </li> <li> <p>Exporters are typically used at the end of a pipeline.</p> </li> </ul>"},{"location":"components/exporters/#custom-exporters","title":"\ud83e\udde0 Custom Exporters","text":"<p>You can easily write your own exporter by implementing the protocol:</p> <pre><code>class CSVExporter:\n    def __init__(self, output_path):\n        self.output_path = output_path\n\n    def export(self, data: List[Dict]) -&gt; None:\n        import csv\n        with open(self.output_path, \"w\") as f:\n            writer = csv.DictWriter(f, fieldnames=data[0].keys())\n            writer.writeheader()\n            writer.writerows(data)\n</code></pre>"},{"location":"components/exporters/#best-practices","title":"\ud83d\udca1 Best Practices","text":"<p>Keep exporters format-agnostic\u2014don\u2019t assume specific label formats.</p>"},{"location":"components/formatters/","title":"\ud83d\udd04 Formatters","text":"<p>Formatters in Chisel convert <code>ChiselRecord</code> objects into the final data formats expected by downstream NLP model training libraries such as PyTorch and HuggingFace \ud83e\udd17 Datasets.</p> <p>They provide a clean separation between data processing and model consumption, making it easy to switch between libraries or pipelines.</p>"},{"location":"components/formatters/#supported-formatters","title":"\u2705 Supported Formatters","text":""},{"location":"components/formatters/#torchdatasetformatter","title":"<code>TorchDatasetFormatter</code>","text":"<p>Converts a list of <code>ChiselRecord</code> instances into a PyTorch-compatible dataset.</p> <p>Output: <code>List[Dict[str, torch.Tensor]]</code></p> <p>Fields Included:</p> <ul> <li> <p><code>input_ids</code></p> </li> <li> <p><code>attention_mask</code></p> </li> <li> <p><code>labels</code></p> </li> </ul> <p>Each record is represented as a dictionary where values are PyTorch tensors, ready to be wrapped in a <code>DataLoader</code>.</p> <pre><code>from chisel.extraction.formatters.torch_formatter import TorchDatasetFormatter\n\nformatter = TorchDatasetFormatter()\ntorch_data = formatter.format(chisel_records)\n</code></pre>"},{"location":"components/formatters/#hfdatasetformatter","title":"HFDatasetFormatter","text":"<p>Converts a list of <code>ChiselRecord</code> instances into a \ud83e\udd17 HuggingFace <code>Dataset</code>.</p> <p>Output: <code>datasets.Dataset</code> object</p> <p>Fields Included: - id - chunk_id - text - input_ids - attention_mask - labels - Optionally: bio_labels if present</p> <p>Usage: <pre><code>from chisel.extraction.formatters.hf_formatter import HFDatasetFormatter\n\nformatter = HFDatasetFormatter()\nhf_dataset = formatter.format(chisel_records)\n</code></pre></p>"},{"location":"components/formatters/#why-formatters","title":"\ud83e\udde9 Why Formatters?","text":"<p>Machine learning frameworks expect specific formats \u2014 not domain-rich objects like ChiselRecord. Formatters handle this final transformation step, letting you:</p> <ul> <li> <p>Stay library-agnostic during preprocessing.</p> </li> <li> <p>Plug in different downstream toolkits easily.</p> </li> <li> <p>Avoid writing custom conversion logic repeatedly.</p> </li> </ul>"},{"location":"components/labelers/","title":"\ud83c\udff7 Labelers","text":"<p>Labelers convert annotated <code>EntitySpan</code> objects into token-level labels, typically in the BIO, BILOU, or Binary formats, which are required for training token classification models (e.g., for NER).</p>"},{"location":"components/labelers/#labeler-interface","title":"\ud83e\uddf1 Labeler Interface","text":"<pre><code>class Labeler(Protocol):\n    subword_strategy: Literal[\"first\", \"all\", \"strict\"] = \"strict\"\n    misalignment_policy: Literal[\"skip\", \"warn\", \"fail\"] = \"skip\"\n\n    def label(self, tokens: List[Token], entities: List[EntitySpan]) -&gt; List[str]:\n        ...\n</code></pre> <p>\u2699\ufe0f Parameters</p> Parameter Description <code>subword_strategy</code> How to label subword tokens:\u2022 <code>\"first\"</code> = label first subword only\u2022 <code>\"all\"</code> = label all subwords\u2022 <code>\"strict\"</code> = label only if token exactly matches entity <code>misalignment_policy</code> How to handle tokens that don\u2019t align with any entity:\u2022 <code>\"skip\"</code> = ignore\u2022 <code>\"warn\"</code> = log a warning\u2022 <code>\"fail\"</code> = raise an error"},{"location":"components/labelers/#built-in-labelers","title":"\ud83e\uddf0 Built-in Labelers","text":""},{"location":"components/labelers/#1-biolabeler","title":"1. BIOLabeler","text":"<p>Applies standard BIO tagging:</p> <p>B-: Beginning of entity <p>I-: Inside entity <p>O: Outside any entity</p>"},{"location":"components/labelers/#2-bilolabeler","title":"2. BILOLabeler","text":"<p>Uses more expressive BILOU tagging:</p> <p>B-: Beginning <p>I-: Inside <p>L-: Last <p>O: Outside</p> <p>U-: Unit (single-token entity)"},{"location":"components/labelers/#3-binarylabeler","title":"3. BinaryLabeler","text":"<p>Simplified format for binary tasks:</p> <p>ENTITY: Any token part of a span</p> <p>O: Outside</p> <p>\ud83e\uddea Example</p> <pre><code>from chisel.labelers.bio import BIOLabeler\n\nlabeler = BIOLabeler(subword_strategy=\"first\", misalignment_policy=\"warn\")\nlabels = labeler.label(tokens, entities)\n# [\"B-PER\", \"I-PER\", \"O\", \"O\", \"B-ORG\", \"I-ORG\"]\n</code></pre>"},{"location":"components/labelers/#subword-behavior","title":"\u26a0\ufe0f Subword Behavior","text":"<p>Subword tokenization can fragment entities:</p> <p>Input text: \"Barack Obama\"</p> <p>Tokens: [\"Bar\", \"##ack\", \"Obama\"]</p> <p>Depending on the subword_strategy:</p> <p>\"first\" \u2192 [\"B-PER\", \"I-PER\", \"O\"]</p> <p>\"all\" \u2192 [\"B-PER\", \"I-PER\", \"I-PER\"]</p> <p>\"strict\" \u2192 will only label if one token covers the full span</p>"},{"location":"components/labelers/#tips","title":"\ud83e\udde0 Tips","text":"<p>BIO/BILOU output is compatible with most token classification models.</p> <p>Use LabelEncoder to convert string labels to integer IDs.</p> <p>For debugging span alignment, use TokenAlignmentValidator.</p>"},{"location":"components/labelers/#labelencoder","title":"\ud83d\udd22 LabelEncoder","text":"<p>The SimpleLabelEncoder is a lightweight utility for converting between string-based labels (e.g. \"B-PER\", \"O\") and integer IDs required by most machine learning frameworks.</p> <p>Unlike typical encoders, this version requires you to pass in the label mapping explicitly at initialization \u2014 making its behavior predictable and immutable.</p>"},{"location":"components/labelers/#features","title":"\ud83e\uddf0 Features","text":"<p>Requires an explicit label_to_id dictionary at initialization.</p> <ul> <li> <p>Converts labels to IDs (encode) and vice versa (decode).</p> </li> <li> <p>Throws helpful errors if unknown labels or IDs are encountered.</p> </li> <li> <p>Can be used internally in export pipelines for Hugging Face and PyTorch compatibility.</p> </li> </ul>"},{"location":"components/labelers/#example","title":"\ud83e\uddea Example","text":"<pre><code>from chisel.extraction.labelers.label_encoder import SimpleLabelEncoder\n\n# Define a label-to-id mapping\nlabel_map = {\n    \"O\": 0,\n    \"B-PER\": 1,\n    \"I-PER\": 2,\n    \"B-LOC\": 3,\n    \"I-LOC\": 4\n}\n\nencoder = SimpleLabelEncoder(label_to_id=label_map)\n\n# Encode a list of labels\nlabel_ids = encoder.encode([\"B-PER\", \"I-PER\", \"O\"])  # \u279d [1, 2, 0]\n\n# Decode a list of label IDs\ndecoded = encoder.decode([1, 2, 0])  # \u279d [\"B-PER\", \"I-PER\", \"O\"]\n</code></pre>"},{"location":"components/labelers/#error-handling","title":"\u26a0\ufe0f Error Handling","text":"<p>If you try to encode or decode unknown values, the encoder raises a clear error:</p> <pre><code>encoder.encode([\"B-ORG\"])  \n# ValueError: Unknown label 'B-ORG' encountered during encoding.\n</code></pre>"},{"location":"components/labelers/#api-reference","title":"\ud83d\udce6 API Reference","text":"Method Description <code>encode(labels)</code> Convert list of label strings to integer IDs <code>decode(ids)</code> Convert list of integer IDs back to label strings <code>get_label_to_id()</code> Return internal label \u2192 ID dictionary <code>get_id_to_label()</code> Return internal ID \u2192 label dictionary"},{"location":"components/models/","title":"\ud83e\uddf1 Data Models in Chisel","text":"<p>Chisel uses a small set of clearly defined, Pydantic-based data models to standardize the representation of annotated texts, tokenization outputs, and aligned spans. These models form the internal \"data language\" that parsers, tokenizers, labelers, and exporters operate on.</p>"},{"location":"components/models/#token","title":"\ud83d\udccd Token","text":"<p>Represents a single token within a text, along with its character-level span.</p> <pre><code>class Token(BaseModel):\n    id: int\n    text: str\n    start: int\n    end: int\n</code></pre> Field Type Description <code>id</code> <code>int</code> Unique identifier (usually index in token list) <code>text</code> <code>str</code> Raw token text <code>start</code> <code>int</code> Character start index in the original text <code>end</code> <code>int</code> Character end index (exclusive) <p>Example:</p> <pre><code>{\n  \"id\": 0,\n  \"text\": \"aspirin\",\n  \"start\": 0,\n  \"end\": 7\n}\n</code></pre>"},{"location":"components/models/#entityspan","title":"\ud83e\udde0 EntitySpan","text":"<p>Represents a labeled span of text extracted from annotations, often prior to tokenization. <pre><code>class EntitySpan(BaseModel):\n    text: str\n    start: int\n    end: int\n    label: str\n    attributes: dict[str, str] = {}\n</code></pre></p> Field Type Description <code>text</code> <code>str</code> The extracted text span <code>start</code> <code>int</code> Start character index <code>end</code> <code>int</code> End character index (exclusive) <code>label</code> <code>str</code> The entity label (e.g. \"DISEASE\", \"ORG\") <code>attributes</code> <code>dict</code> Optional metadata associated with the span"},{"location":"components/models/#tokenentityspan","title":"\ud83d\udd17 TokenEntitySpan","text":"<p>Aligns an EntitySpan to its token-level representation. Used after tokenization and alignment. <pre><code>class TokenEntitySpan(BaseModel):\n    entity: EntitySpan\n    token_indices: List[int]\n</code></pre></p> Field Type Description <code>entity</code> <code>EntitySpan</code> The original span <code>token_indices</code> <code>List[int]</code> Indices into the <code>Token</code> list that align with this span <p>This format is useful for converting between span-level and sequence-label formats like BIO/BILOU.</p>"},{"location":"components/models/#chiselrecord","title":"\ud83e\uddfe ChiselRecord","text":"<p>A central container for all relevant information about a processed text segment. Used throughout pipelines and by all exporters.</p> <pre><code>class ChiselRecord(BaseModel):\n    id: str\n    chunk_id: int\n    text: str\n    tokens: List[Token]\n    entities: List[EntitySpan]\n    bio_labels: Optional[List[str]] = Field(default=None, alias=\"bio-labels\")\n    labels: Optional[List[int]] = None\n    input_ids: Optional[List[int]] = None\n    attention_mask: Optional[List[int]] = None\n</code></pre> Field Type Description <code>id</code> <code>str</code> Unique document ID <code>chunk_id</code> <code>int</code> Unique ID for this chunk of the document <code>text</code> <code>str</code> The original or preprocessed text <code>tokens</code> <code>List[Token]</code> Tokenized representation of the text <code>entities</code> <code>List[EntitySpan]</code> Extracted entities in character span format <code>bio_labels</code> <code>Optional[List[str]]</code> BIO/BILOU labels (one per token) <code>labels</code> <code>Optional[List[int]]</code> Encoded integer labels <code>input_ids</code> <code>Optional[List[int]]</code> Tokenizer output for transformer input <code>attention_mask</code> <code>Optional[List[int]]</code> Attention mask corresponding to input_ids <p>Example: <pre><code>{\n  \"id\": \"doc123\",\n  \"chunk_id\": 0,\n  \"text\": \"Aspirin is used to treat pain.\",\n  \"tokens\": [...],\n  \"entities\": [...],\n  \"bio_labels\": [\"B-DRUG\", \"O\", \"O\", \"O\", \"O\"],\n  \"labels\": [1, 0, 0, 0, 0],\n  \"input_ids\": [101, 1234, 2003, ...],\n  \"attention_mask\": [1, 1, 1, ...]\n}\n</code></pre></p>"},{"location":"components/parsers/","title":"\ud83e\udde9 Parsers","text":"<p>Parsers are the entry point to the Chisel pipeline. They take raw annotated documents and extract the cleaned text and a list of entity spans (<code>EntitySpan</code>) with their character offsets.</p> <p>Each parser implements the following protocol:</p> <pre><code>class Parser(Protocol):\n    def parse(self, doc: str) -&gt; tuple[str, List[EntitySpan]]:\n        ...\n</code></pre>"},{"location":"components/parsers/#available-parsers","title":"\ud83e\uddf1 Available Parsers","text":""},{"location":"components/parsers/#1-htmltagparser","title":"1. HTMLTagParser","text":"<p>Parses entity spans from non-standard HTML/XML tags like , , or custom tags like . <p>\ud83e\uddea Example Input <pre><code>The &lt;ORG&gt;UN&lt;/ORG&gt; met with &lt;PER&gt;Joe Biden&lt;/PER&gt; today.\n</code></pre></p> <p>\u2705 Output</p> <p>Cleaned Text: <code>\"The UN met with Joe Biden today.\"</code></p> <p><code>EntitySpan(label=\"ORG\", start=4, end=6, text=\"UN\")</code></p> <p><code>EntitySpan(label=\"PER\", start=16, end=25, text=\"Joe Biden\")</code></p> <p>\ud83d\udd27 Parameters</p> Name Description <code>label_strategy</code> <code>\"tag\"</code> to use the tag name as label (e.g., <code>PER</code>) or <code>\"attribute\"</code> to extract a specific HTML attribute as the label <code>attribute_name</code> Used if <code>label_strategy=\"attribute\"</code> \u2014 specifies which attribute to use as label <code>allow_nested</code> If <code>True</code>, allows nested tags and creates spans for each. If <code>False</code>, only outermost span is retained"},{"location":"components/parsers/#2-jsonspanparser","title":"2. JSONSpanParser","text":"<p>To be written. There is an example implementation in the examples folder on git.</p>"},{"location":"components/parsers/#3-conllparser","title":"3. ConllParser","text":"<p>To be written. There is an example implementation in the examples folder on git.</p>"},{"location":"components/parsers/#notes","title":"\ud83e\udde0 Notes","text":"<p>Parsers are intentionally minimal and decoupled from tokenization.</p> <p>You can easily extend Chisel by writing your own parser, e.g., for PDFs, docx files, or domain-specific formats.</p> <p>All parsers return spans using start/end character offsets on the cleaned version of the text.</p>"},{"location":"components/parsers/#custom-parsers","title":"\u2795 Custom Parsers","text":"<p>To implement your own parser, simply conform to the protocol:</p> <pre><code>class MyCustomParser:\n    def parse(self, doc: str) -&gt; Tuple[str, List[EntitySpan]]:\n        # 1. Extract raw text\n        # 2. Identify annotated spans with start, end, label\n        # 3. Return cleaned text and EntitySpan list\n        ...\n</code></pre>"},{"location":"components/tokenizers/","title":"\u2702\ufe0f Tokenizers","text":"<p>Tokenizers split raw text into smaller units (tokens) and assign each token: - The text (string form of the token) - The start and end character offsets in the original text - The token ID (usually the vocabulary index from a model tokenizer)</p> <p>Chisel uses tokenizers to align character-level entity spans with model-compatible tokens for tasks like Named Entity Recognition (NER).</p>"},{"location":"components/tokenizers/#tokenizer-interface","title":"\ud83e\uddf1 Tokenizer Interface","text":"<p>Each tokenizer implements the following protocol:</p> <pre><code>class Tokenizer(Protocol):\n    def tokenize(self, text: str) -&gt; List[Token]:\n        ...\n</code></pre> <p>Where Token is:</p> <pre><code>class Token(BaseModel):\n    id: int       # model vocab ID\n    text: str     # token string\n    start: int    # start char index\n    end: int      # end char index\n</code></pre>"},{"location":"components/tokenizers/#built-in-tokenizers","title":"\ud83e\uddf0 Built-in Tokenizers","text":""},{"location":"components/tokenizers/#1-huggingfacetokenizer","title":"1. HuggingFaceTokenizer","text":"<p>Wraps any pretrained Hugging Face tokenizer (AutoTokenizer) and outputs properly aligned Token objects.</p> <p>\ud83d\udd27 Parameters</p> Name Description <code>model_id</code> Pretrained model name (e.g., <code>\"bert-base-uncased\"</code>, <code>\"distilroberta-base\"</code>) <p>\ud83e\uddea Example <pre><code>from chisel.tokenizers.hf_tokenizer import HuggingFaceTokenizer\n\ntokenizer = HuggingFaceTokenizer(\"bert-base-uncased\")\ntokens = tokenizer.tokenize(\"Barack Obama was president.\")\n</code></pre> Returns a list of Token objects with offsets and token IDs.</p>"},{"location":"components/tokenizers/#tokenizer-behavior","title":"\u26a0\ufe0f Tokenizer Behavior","text":"<p>Different tokenizers use different subword strategies:</p> <p>WordPiece (e.g., BERT): breaks unknown words into fragments with ## prefix</p> <p>BPE (e.g., RoBERTa, GPT2): breaks based on byte-pair frequencies, often splits tokens at character level</p> <p>SentencePiece (e.g., ALBERT, T5): learned segmentation of raw text with special prefix tokens like \u2581</p> <p>These affect how span alignment and labeling must be handled. Chisel supports multiple subword alignment strategies (see Labelers) to accommodate this.</p>"},{"location":"components/tokenizers/#tips","title":"\ud83e\udde0 Tips","text":"<p>Use tokenizer.tokenize() when creating datasets or debugging span alignment.</p> <p>The returned tokens are automatically compatible with labelers and chunkers.</p> <p>You can test tokenizer behavior on edge cases using the tests/test_tokenizers suite.</p>"},{"location":"components/tokenizers/#custom-tokenizer","title":"\u2795 Custom Tokenizer","text":"<p>To create your own tokenizer, simply implement the protocol:</p> <pre><code>class MyCustomTokenizer:\n    def tokenize(self, text: str) -&gt; List[Token]:\n        # Custom logic\n        ...\n</code></pre> <p>Next up: \ud83c\udff7 Labelers \u2192</p>"},{"location":"components/validators/","title":"\u2705 Validators","text":"<p>Validators in Chisel help catch inconsistencies early in the preprocessing pipeline. They provide a flexible validation system to help catch annotation errors early \u2014 before they silently degrade your model performance.</p>"},{"location":"components/validators/#why-validators","title":"Why Validators?","text":"<p>When training token classification models, small inconsistencies in span alignment or label sequences can lead to misleading results, unstable training, or even complete failures downstream. Validators give you an explicit place to catch and handle these inconsistencies.</p> <p>For example, if a span is misaligned with token boundaries or if a label sequence does not match the expected entity structure, a validator can flag it \u2014 and depending on configuration, either raise an error or continue gracefully.</p> <p>They are divided into two categories:</p>"},{"location":"components/validators/#parse-validators","title":"\ud83d\udd0d Parse Validators","text":"<p>Parse validators run before tokenization and ensure that the entity spans extracted by the parser are accurate with respect to the original text.</p>"},{"location":"components/validators/#protocol","title":"\ud83e\udde9 Protocol","text":"<pre><code>@runtime_checkable\nclass ParseValidator(Protocol):\n    on_error: Literal[\"warn\", \"raise\"]\n\n    def validate(self, text: str, span: EntitySpan) -&gt; None: ...\n</code></pre>"},{"location":"components/validators/#implementation-defaultparsevalidator","title":"\ud83d\udee0\ufe0f Implementation: <code>DefaultParseValidator</code>","text":"<pre><code>class DefaultParseValidator(ParseValidator):\n    \"\"\"\n    - Checks if entity text exists in the full text.\n    - Checks if `entity.text` matches `text[start:end]`.\n    - Checks for valid index boundaries.\n    - Warns or raises on errors based on `on_error`.\n    \"\"\"\n</code></pre>"},{"location":"components/validators/#behavior","title":"\ud83d\udd27 Behavior","text":"Check Description <code>entity.text in text</code> Ensures the string exists in the input text. <code>text[start:end] == entity.text</code> Confirms character indices match the string. <code>0 &lt;= start &lt; end &lt;= len(text)</code> Validates index boundaries. Empty <code>text</code> Raises or warns on empty span texts."},{"location":"components/validators/#usage","title":"\ud83d\udca1 Usage","text":"<pre><code>validator = DefaultParseValidator(on_error=\"raise\")\nvalidator.validate(text, span)\n</code></pre>"},{"location":"components/validators/#token-alignment-validators","title":"\ud83e\uddf7 Token Alignment Validators","text":"<p>These validators ensure that token-aligned entity spans can be reconstructed accurately from the token IDs using the tokenizer.</p>"},{"location":"components/validators/#protocol_1","title":"\ud83e\udde9 Protocol","text":"<pre><code>@runtime_checkable\nclass TokenAlignmentValidator(Protocol):\n    on_error: Literal[\"warn\", \"raise\"]\n\n    def validate(\n        self, tokens: List[Token], span: TokenEntitySpan\n    ) -&gt; None: ...\n</code></pre>"},{"location":"components/validators/#implementation-hftokenalignmentvalidator","title":"\ud83d\udee0\ufe0f Implementation: <code>HFTokenAlignmentValidator</code>","text":"<pre><code>class HFTokenAlignmentValidator(TokenAlignmentValidator):\n    \"\"\"\n    - Tokenizes `entity.text` and decodes it.\n    - Compares to decoded token IDs from aligned tokens.\n    - Warns or raises if mismatch is found.\n    \"\"\"\n</code></pre>"},{"location":"components/validators/#behavior_1","title":"\ud83d\udd27 Behavior","text":"Check Description Decode <code>entity.text</code> Using HuggingFace tokenizer (<code>add_special_tokens=False</code>). Decode token span using IDs From <code>TokenEntitySpan.token_indices</code>. Compare decoded strings If mismatch, either warn or raise."},{"location":"components/validators/#usage_1","title":"\ud83d\udca1 Usage","text":"<pre><code>from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\nvalidator = HFTokenAlignmentValidator(tokenizer=tokenizer, on_error=\"warn\")\nvalidator.validate(tokens, span)\n</code></pre>"},{"location":"components/validators/#on_error-behavior","title":"\u26a0\ufe0f on_error Behavior","text":"<p>All validators accept an on_error argument:</p> Value Effect <code>\"raise\"</code> Raises an exception immediately (strict mode). <code>\"warn\"</code> Logs a warning and continues execution. <p>This allows you to use strict checks during development, but run more flexibly during large-scale preprocessing.</p>"},{"location":"components/validators/#skipping-only-the-broken-spans","title":"Skipping Only the Broken Spans","text":"<pre><code>valid_spans = []\nfor span in token_entity_spans:\n    try:\n        validator.validate(tokens, span)\n        valid_spans.append(span)\n    except ValueError:\n        continue  # Skip just this one\n</code></pre>"},{"location":"components/validators/#implementing-custom-validators","title":"\ud83d\udee0\ufe0f Implementing Custom Validators","text":"<p>You can easily extend Chisel by writing your own validator classes that follow the appropriate protocol.</p>"},{"location":"components/validators/#custom-parsevalidator","title":"\u2705 Custom <code>ParseValidator</code>","text":"<p>To write a custom validator that checks the parsed spans before tokenization, inherit from or implement the ParseValidator protocol:</p> <pre><code>from chisel.extraction.base.protocols import ParseValidator\nfrom chisel.extraction.models import EntitySpan\n\nclass NoOverlappingSpansValidator(ParseValidator):\n    \"\"\"\n    Ensures that no two entity spans overlap in character space.\n    \"\"\"\n\n    def __init__(self, on_error: Literal[\"warn\", \"raise\"] = \"warn\"):\n        self.on_error = on_error\n\n    def validate(self, text: str, entities: list[EntitySpan]) -&gt; None:\n        sorted_entities = sorted(entities, key=lambda e: e.start)\n        for i in range(len(sorted_entities) - 1):\n            if sorted_entities[i].end &gt; sorted_entities[i + 1].start:\n                msg = (\n                    f\"Overlapping spans: '{sorted_entities[i]}' \"\n                    f\"and '{sorted_entities[i + 1]}'\"\n                )\n                if self.on_error == \"warn\":\n                    print(\"Warning:\", msg)\n                else:\n                    raise ValueError(msg)\n</code></pre>"},{"location":"components/validators/#custom-tokenalignmentvalidator","title":"\u2705 Custom TokenAlignmentValidator","text":"<p>To write a validator that checks alignment between tokens and entities, use the TokenAlignmentValidator protocol:</p> <pre><code>from chisel.extraction.base.protocols import TokenAlignmentValidator\nfrom chisel.extraction.models import Token, TokenEntitySpan\n\nclass NoEmptySpanValidator(TokenAlignmentValidator):\n    \"\"\"\n    Ensures no TokenEntitySpan has zero token indices.\n    \"\"\"\n\n    def __init__(self, on_error: Literal[\"warn\", \"raise\"] = \"warn\"):\n        self.on_error = on_error\n\n    def validate(\n        self,\n        tokens: list[Token],\n        token_entity_spans: list[TokenEntitySpan],\n    ) -&gt; None:\n        for span in token_entity_spans:\n            if not span.token_indices:\n                msg = f\"TokenEntitySpan for '{span.entity.text}' has no token indices.\"\n                if self.on_error == \"warn\":\n                    print(\"Warning:\", msg)\n                else:\n                    raise ValueError(msg)\n</code></pre>"},{"location":"components/validators/#tip-chain-multiple-validators","title":"\ud83e\uddea Tip: Chain Multiple Validators","text":"<p>You can combine multiple validators in your pipeline like so: <pre><code>parse_validators = [\n    DefaultParseValidator(on_error=\"raise\"),\n    NoOverlappingSpansValidator(on_error=\"warn\"),\n]\n\nfor validator in parse_validators:\n    validator.validate(text, entities)\n</code></pre></p> <p>This design encourages small, composable units of validation logic.</p>"},{"location":"examples/conll/","title":"\ud83e\uddea Example: Processing CoNLL NER Data with Chisel","text":"<p>This example demonstrates how to parse the CoNLL-2003 dataset into Chisel's internal ChiselRecord format, suitable for training transformer-based token classification models.</p>"},{"location":"examples/conll/#step-1-download-conll-data","title":"\ud83d\udce5 Step 1: Download CoNLL Data","text":"<p>We use the version hosted by the CrossWeigh repository.</p> <pre><code>import requests\n\nurl = \"https://raw.githubusercontent.com/ZihanWangKi/CrossWeigh/refs/heads/master/data/conllpp_train.txt\"\nresponse = requests.get(url)\ndocs = response.text.split(\"-DOCSTART- -X- -X- O\\n\\n\")\ndocs = list(filter(lambda x: len(x) &gt; 0, docs))\n</code></pre>"},{"location":"examples/conll/#step-2-define-a-parser-for-conll-format","title":"\ud83e\udde9 Step 2: Define a Parser for CoNLL Format","text":"<p>Note: Whilst this may seem a bit complicated at first glance, with help of generative AI and the validators chisel provide, it should be fairly quick to write own custom parsers.</p> <pre><code>from typing import Tuple, List\nfrom chisel.extraction.base.protocols import Parser\nfrom chisel.extraction.models.models import EntitySpan\nimport string\n\nclass ConllParser(Parser):\n    def parse(self, doc: str) -&gt; Tuple[str, List[EntitySpan]]:\n        tokens, labels = [], []\n        for line in doc.strip().splitlines():\n            if not line.strip():\n                continue\n            splits = line.strip().split(\" \")\n            tokens.append(splits[0])\n            labels.append(splits[-1])\n\n        text = \"\"\n        spans = []\n        char_offset = 0\n        i = 0\n\n        while i &lt; len(tokens):\n            token = tokens[i]\n            label = labels[i]\n\n            if text and token not in string.punctuation:\n                text += \" \"\n                char_offset += 1\n\n            if label.startswith(\"B-\"):\n                ent_label = label[2:]\n                ent_start = char_offset\n                ent_text = token\n                text += token\n                char_offset += len(token)\n                i += 1\n                while i &lt; len(tokens) and labels[i].startswith(\"I-\"):\n                    text += \" \" + tokens[i]\n                    ent_text += \" \" + tokens[i]\n                    char_offset += 1 + len(tokens[i])\n                    i += 1\n                ent_end = char_offset\n                spans.append(EntitySpan(text=ent_text, start=ent_start, end=ent_end, label=ent_label))\n            else:\n                text += token\n                char_offset += len(token)\n                i += 1\n\n        return text.strip(), spans\n</code></pre>"},{"location":"examples/conll/#step-3-initialize-chisel-components","title":"\ud83d\udd27 Step 3: Initialize Chisel Components","text":"<pre><code>from transformers import AutoTokenizer\nfrom chisel.extraction.tokenizers.hf_tokenizer import HFTokenizer\nfrom chisel.extraction.span_aligners.token_span_aligner import TokenSpanAligner\nfrom chisel.extraction.labelers.bio_labeler import BIOLabeler\nfrom chisel.extraction.labelers.label_encoder import SimpleLabelEncoder\nfrom chisel.extraction.validators.validators import DefaultParseValidator, HFTokenAlignmentValidator\nfrom chisel.extraction.formatters.hf_formatter import HFDatasetFormatter\nfrom chisel.extraction.models.models import ChiselRecord\n</code></pre>"},{"location":"examples/conll/#components","title":"\ud83d\udce6 Components","text":"<pre><code>parser = ConllParser()\ntokenizer = HFTokenizer(model_name=\"bert-base-cased\")\naligner = TokenSpanAligner()\nlabeler = BIOLabeler()\nlabel_encoder = SimpleLabelEncoder(label_to_id={\n    'O': 0,\n    'B-ORG': 1,\n    'I-ORG': 2,\n    'B-PER': 3,\n    'I-PER': 4,\n    'B-MISC': 5,\n    'I-MISC': 6,\n    'B-LOC': 7,\n    'I-LOC': 8\n})\n\nparse_validators = [DefaultParseValidator(on_error=\"raise\")]\nlabel_validators = [HFTokenAlignmentValidator(tokenizer=tokenizer.tokenizer, on_error=\"raise\")]\nformatter = HFDatasetFormatter()\n</code></pre>"},{"location":"examples/conll/#step-4-run-the-pipeline","title":"\ud83d\udd04 Step 4: Run the Pipeline","text":"<pre><code>processed_data = []\n\n# \ud83d\udd01 Pipeline loop\nfor idx, example in enumerate(docs):\n    text, entities = parser.parse(example)\n\n    # \ud83e\uddea Per-span validation \u2014 skip bad spans\n    valid_spans = []\n    for span in entities:\n        try:\n            for validator in parse_validators:\n                validator.validate(text, span)\n            valid_spans.append(span)\n        except ValueError:\n            continue \n\n    tokens = tokenizer.tokenize(text)\n    token_entity_spans = aligner.align(entities, tokens)\n\n    labels = labeler.label(tokens, token_entity_spans)\n    encoded_labels = label_encoder.encode(labels)\n\n    # \ud83e\uddea Per-span validation \u2014 skip bad spans\n    valid_token_spans = []\n    for span in token_entity_spans:\n        try:\n            for validator in label_validators:\n                validator.validate(tokens, span)\n            valid_token_spans.append(span)\n        except ValueError:\n            continue  # Optionally log or collect stats on dropped spans\n\n    record = ChiselRecord(\n                id=str(idx),\n                chunk_id=0,\n                text=tokenizer.tokenizer.decode([token.id for token in tokens]),\n                tokens=tokens,\n                input_ids=[token.id for token in tokens],\n                attention_mask=[1] * len(tokens),\n                entities=[tes.entity for tes in valid_token_spans],\n                bio_labels=labels,\n                labels=encoded_labels\n            )\n    processed_data.append(record)\n\ndata = formatters.format(processed_data)\n</code></pre>"},{"location":"examples/conll/#output","title":"\u2705 Output","text":"<p>You now have a HuggingFace dataset ready!</p>"},{"location":"examples/json/","title":"\ud83d\udc57 Example: Processing Fashion Brand NER (JSON Format) with Chisel","text":"<p>This example shows how to preprocess the explosion/ner-fashion-brands dataset into ChiselRecord objects for training transformer-based NER models using BILO labeling.</p>"},{"location":"examples/json/#step-1-load-the-dataset","title":"\ud83d\udce5 Step 1: Load the Dataset","text":"<pre><code>from datasets import load_dataset\n\nds = load_dataset(\"explosion/ner-fashion-brands\")\n</code></pre>"},{"location":"examples/json/#step-2-implement-a-json-span-parser","title":"\ud83e\udde9 Step 2: Implement a JSON Span Parser","text":"<p>The dataset provides character-level spans in a spans field. We write a parser that extracts these into Chisel's EntitySpan format.</p> <pre><code>from typing import Tuple, List\nfrom chisel.extraction.base.protocols import Parser\nfrom chisel.extraction.models.models import EntitySpan\n\nclass JSONSpanParser(Parser):\n    def parse(self, doc: dict) -&gt; Tuple[str, List[EntitySpan]]:\n        text = doc[\"text\"]\n        entities = [\n            EntitySpan(\n                text=text[e[\"start\"]:e[\"end\"]],\n                start=e[\"start\"],\n                end=e[\"end\"],\n                label=e[\"label\"]\n            )\n            for e in doc.get(\"spans\", [])\n        ]\n        return text, entities\n</code></pre>"},{"location":"examples/json/#step-3-initialize-chisel-components","title":"\ud83d\udd27 Step 3: Initialize Chisel Components","text":"<pre><code>from chisel.extraction.tokenizers.hf_tokenizer import HFTokenizer\nfrom chisel.extraction.span_aligners.token_span_aligner import TokenSpanAligner\nfrom chisel.extraction.labelers.bilo_labeler import BILOLabeler\nfrom chisel.extraction.labelers.label_encoder import SimpleLabelEncoder\nfrom chisel.extraction.validators.validators import DefaultParseValidator, HFTokenAlignmentValidator\nfrom chisel.extraction.formatters.torch_formatter import TorchDatasetFormatter\nfrom chisel.extraction.models.models import ChiselRecord\n</code></pre>"},{"location":"examples/json/#component-setup","title":"Component Setup","text":"<pre><code>parser = JSONSpanParser()\ntokenizer = HFTokenizer(model_name=\"bert-base-cased\")\naligner = TokenSpanAligner()\nlabeler = BILOLabeler()\n\nlabel_encoder = SimpleLabelEncoder(label_to_id={\n    'O': 0,\n    'B-FASHION_BRAND': 1,\n    'I-FASHION_BRAND': 2,\n    'L-FASHION_BRAND': 3,\n    'U-FASHION_BRAND': 4,\n})\n\nparse_validators = [DefaultParseValidator(on_error=\"raise\")]\nlabel_validators = [HFTokenAlignmentValidator(tokenizer=tokenizer.tokenizer, on_error=\"raise\")]\nformatter = TorchDatasetFormatter()\n</code></pre>"},{"location":"examples/json/#step-4-run-the-preprocessing-pipeline","title":"\ud83d\udd04 Step 4: Run the Preprocessing Pipeline","text":"<pre><code>processed_data = []\n\nfor idx, example in enumerate(ds[\"train\"]):\n    text, entities = parser.parse(example)\n\n    # \ud83e\uddea Per-span validation \u2014 skip bad spans\n    valid_spans = []\n    for span in entities:\n        try:\n            for validator in parse_validators:\n                validator.validate(text, span)\n            valid_spans.append(span)\n        except ValueError:\n            continue \n\n    tokens = tokenizer.tokenize(text)\n    token_entity_spans = aligner.align(entities, tokens)\n\n    labels = labeler.label(tokens, token_entity_spans)\n    encoded_labels = label_encoder.encode(labels)\n\n    # \ud83e\uddea Per-span validation \u2014 skip bad spans\n    valid_token_spans = []\n    for span in token_entity_spans:\n        try:\n            for validator in label_validators:\n                validator.validate(tokens, span)\n            valid_token_spans.append(span)\n        except ValueError:\n            continue  # Optionally log or collect stats on dropped spans\n\n    record = ChiselRecord(\n        id=str(idx),\n        chunk_id=0,\n        text=tokenizer.tokenizer.decode([token.id for token in tokens]),\n        tokens=tokens,\n        input_ids=[token.id for token in tokens],\n        attention_mask=[1] * len(tokens),\n        entities=[tes.entity for tes in valid_token_spans],\n        bio_labels=labels,\n        labels=encoded_labels\n    )\n    processed_data.append(record)\n\ndata = formatter.format(processed_data)\n</code></pre>"},{"location":"examples/json/#output","title":"\u2705 Output","text":"<p>You now have a torch dataset ready for training!</p>"},{"location":"examples/ncbi/","title":"\ud83e\uddea Example: Processing the NCBI Disease Dataset with Chisel","text":"<p>This example demonstrates how to preprocess the NCBI Disease Corpus using the Chisel library, transforming the data into a format suitable for transformer-based token classification models.</p>"},{"location":"examples/ncbi/#step-1-download-the-ncbi-dataset","title":"\ud83d\udce5 Step 1: Download the NCBI Dataset","text":"<p>The NCBI corpus uses inline HTML-like tags to annotate disease mentions. Annotations look like:</p> <pre><code>&lt;category=\"SpecificDisease\"&gt;Cancer&lt;/category&gt;\n</code></pre> <p>To work with this format, we ensure it is valid XML by renaming attributes to:</p> <pre><code>&lt;category category=\"SpecificDisease\"&gt;Cancer&lt;/category&gt;\n</code></pre>"},{"location":"examples/ncbi/#download-and-extract-the-dataset","title":"\u2705 Download and extract the dataset","text":"<pre><code>import requests, zipfile, io, os\n\nurl = \"https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/NCBI_corpus.zip\"\nextract_to = \"./data\"\nos.makedirs(extract_to, exist_ok=True)\n\nresponse = requests.get(url)\nif response.status_code == 200:\n    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(f\"Extracted contents to: {extract_to}\")\nelse:\n    print(f\"Failed to download. Status code: {response.status_code}\")\n</code></pre>"},{"location":"examples/ncbi/#step-2-load-and-clean-the-data","title":"\ud83d\udcc4 Step 2: Load and Clean the Data","text":"<pre><code>annotations = []\nwith open(\"./data/NCBI_corpus_training.txt\", \"r\") as f:\n    for line in f:\n        splits = line.split(\"\\t\")\n        annotations.append({\n            \"id\": splits[0].strip(),\n            \"text\": \" \".join(splits[1:]).strip().replace('&lt;category=\"', '&lt;category category=\"')\n        })\n</code></pre>"},{"location":"examples/ncbi/#step-3-preprocess-with-chisel","title":"\ud83e\uddf1 Step 3: Preprocess with Chisel","text":"<p>We define and connect the pipeline components from Chisel.</p>"},{"location":"examples/ncbi/#setup","title":"\ud83d\udd27 Setup","text":"<pre><code>from transformers import AutoTokenizer\nfrom chisel.extraction.parsers.html_tag_parser import HTMLTagParser\nfrom chisel.extraction.tokenizers.hf_tokenizer import HFTokenizer\nfrom chisel.extraction.chunkers.fixed_length_chunker import FixedLengthTokenChunker\nfrom chisel.extraction.span_aligners.token_span_aligner import TokenSpanAligner\nfrom chisel.extraction.labelers.bio_labeler import BIOLabeler\nfrom chisel.extraction.labelers.label_encoder import SimpleLabelEncoder\nfrom chisel.extraction.validators.validators import DefaultParseValidator, HFTokenAlignmentValidator\nfrom chisel.extraction.formatters.torch_formatter import TorchDatasetFormatter\nfrom chisel.extraction.models.models import ChiselRecord\n</code></pre>"},{"location":"examples/ncbi/#components","title":"\ud83e\udde9 Components","text":"<pre><code>parser = HTMLTagParser(label_strategy=\"attribute\", attribute_name=\"category\")\ntokenizer = HFTokenizer(model_name=\"bert-base-cased\")\naligner = TokenSpanAligner()\nchunker = FixedLengthTokenChunker(max_tokens=512, overlap=0)\nlabeler = BIOLabeler()\nlabel_encoder = SimpleLabelEncoder(label_to_id={\n    'O': 0,\n    'B-Modifier': 1,\n    'I-Modifier': 2,\n    'B-SpecificDisease': 3,\n    'I-SpecificDisease': 4,\n    'B-CompositeMention': 5,\n    'I-CompositeMention': 6,\n    'B-DiseaseClass': 7,\n    'I-DiseaseClass': 8,\n})\nparse_validators = [DefaultParseValidator(on_error=\"raise\")]\nlabel_validators = [HFTokenAlignmentValidator(tokenizer=tokenizer.tokenizer, on_error=\"raise\")]\nformatter = TorchDatasetFormatter()\n</code></pre>"},{"location":"examples/ncbi/#step-4-run-the-pipeline","title":"\ud83d\udd04 Step 4: Run the Pipeline","text":"<pre><code>processed_data = []\n\n# \ud83d\udd01 Pipeline loop\nfor example in annotations:\n    text, entities = parser.parse(example[\"text\"])\n\n    # \ud83e\uddea Per-span validation \u2014 skip bad spans\n    valid_spans = []\n    for span in entities:\n        try:\n            for validator in parse_validators:\n                validator.validate(text, span)\n            valid_spans.append(span)\n        except ValueError:\n            continue \n\n    tokens = tokenizer.tokenize(text)\n    token_entity_spans = aligner.align(entities, tokens)\n\n    token_chunks, entity_chunks = chunker.chunk(tokens, token_entity_spans)\n\n    for chunk_id, (toks, ents) in enumerate(zip(token_chunks, entity_chunks)):\n\n        labels = labeler.label(toks, ents)\n        encoded_labels = label_encoder.encode(labels)\n\n        # \ud83e\uddea Per-span validation \u2014 skip bad spans\n        valid_token_spans = []\n        for span in token_entity_spans:\n            try:\n                for validator in label_validators:\n                    validator.validate(tokens, span)\n                valid_token_spans.append(span)\n            except ValueError as e:\n                print(f\"Validation error: {e}\")  # Log the error for debugging\n                continue  # Optionally log or collect stats on dropped spans\n\n        record = ChiselRecord(\n                id=example[\"id\"],\n                chunk_id=chunk_id,\n                text=tokenizer.tokenizer.decode([token.id for token in toks]),\n                tokens=toks,\n                input_ids=[token.id for token in toks],\n                attention_mask=[1] * len(toks),\n                entities=[tes.entity for tes in valid_token_spans],\n                bio_labels=labels,\n                labels=encoded_labels\n            )\n        processed_data.append(record)\n\ndata = formatter.format(processed_data)\n</code></pre>"},{"location":"examples/ncbi/#output","title":"\u2705 Output","text":"<p>You now have a Torch dataset ready for analysis!</p>"},{"location":"reference/chisel/","title":"API Reference","text":""}]}