{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb156f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/workspaces/chisel/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783042a",
   "metadata": {},
   "source": [
    "# üß™ Example: Processing CoNLL NER Data with Chisel\n",
    "This example demonstrates how to parse the CoNLL-2003 dataset into Chisel's internal ChiselRecord format, suitable for training transformer-based token classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b86656",
   "metadata": {},
   "source": [
    "## üì• Step 1: Download CoNLL Data\n",
    "We use the version hosted by the [CrossWeigh](https://github.com/ZihanWangKi/CrossWeigh) repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ecdcd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/ZihanWangKi/CrossWeigh/refs/heads/master/data/conllpp_train.txt\"\n",
    "response = requests.get(url)\n",
    "docs = response.text.split(\"-DOCSTART- -X- -X- O\\n\\n\")\n",
    "docs = list(filter(lambda x: len(x) > 0, docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c4a56",
   "metadata": {},
   "source": [
    "## üß© Step 2: Define a Parser for CoNLL Format\n",
    "Note: Whilst this may seem a bit complicated at first glance, with help of generative AI and the validators chisel provide, it should be fairly quick to write own custom parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8995a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from chisel.extraction.base.protocols import Parser\n",
    "from chisel.extraction.models.models import EntitySpan\n",
    "import string\n",
    "\n",
    "\n",
    "class ConllParser(Parser):\n",
    "    def parse(self, doc: str) -> Tuple[str, List[EntitySpan]]:\n",
    "        tokens, labels = [], []\n",
    "        for line in doc.strip().splitlines():\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            splits = line.strip().split(\" \")\n",
    "            tokens.append(splits[0])\n",
    "            labels.append(splits[-1])\n",
    "\n",
    "        text = \"\"\n",
    "        spans = []\n",
    "        char_offset = 0\n",
    "        i = 0\n",
    "\n",
    "        while i < len(tokens):\n",
    "            token = tokens[i]\n",
    "            label = labels[i]\n",
    "\n",
    "            # Only add joiner if previous token exists and current token is not punctuation\n",
    "            if text and token not in string.punctuation:\n",
    "                text += \" \"\n",
    "                char_offset += len(\" \")\n",
    "\n",
    "            if label.startswith(\"B-\"):\n",
    "                ent_label = label[2:]\n",
    "                ent_start = char_offset\n",
    "                ent_text = token\n",
    "                text += token\n",
    "                char_offset += len(token)\n",
    "                i += 1\n",
    "                while i < len(tokens) and labels[i].startswith(\"I-\"):\n",
    "                    text += \" \" + tokens[i]\n",
    "                    ent_text += \" \" + tokens[i]\n",
    "                    char_offset += len(\" \") + len(tokens[i])\n",
    "                    i += 1\n",
    "                ent_end = char_offset\n",
    "                spans.append(\n",
    "                    EntitySpan(\n",
    "                        text=ent_text, start=ent_start, end=ent_end, label=ent_label\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                text += token\n",
    "                char_offset += len(token)\n",
    "                i += 1\n",
    "        return text.strip(), spans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a8812b",
   "metadata": {},
   "source": [
    "## üîß Step 3: Initialize Chisel Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64396e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Imports\n",
    "from transformers import AutoTokenizer\n",
    "from chisel.extraction.tokenizers.hf_tokenizer import HFTokenizer\n",
    "from chisel.extraction.span_aligners.token_span_aligner import TokenSpanAligner\n",
    "from chisel.extraction.labelers.bio_labeler import BIOLabeler\n",
    "from chisel.extraction.labelers.label_encoder import SimpleLabelEncoder\n",
    "from chisel.extraction.validators.validators import DefaultParseValidator, HFTokenAlignmentValidator\n",
    "from chisel.extraction.formatters.hf_formatter import HFDatasetFormatter\n",
    "from chisel.extraction.models.models import ChiselRecord\n",
    "from chisel.extraction.models.models import EntitySpan\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f945d",
   "metadata": {},
   "source": [
    "## üì¶ Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e48d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ConllParser()\n",
    "tokenizer = HFTokenizer(model_name=\"bert-base-cased\")\n",
    "aligner = TokenSpanAligner()\n",
    "labeler = BIOLabeler()\n",
    "label_encoder = SimpleLabelEncoder(label_to_id={\n",
    " 'O': 0,\n",
    " 'B-ORG': 1,\n",
    " 'I-ORG': 2,\n",
    " 'B-PER': 3,\n",
    " 'I-PER': 4,\n",
    " 'B-MISC': 5,\n",
    " 'I-MISC': 6,\n",
    " 'B-LOC': 7,\n",
    " 'I-LOC': 8\n",
    "})\n",
    "\n",
    "parse_validators = [DefaultParseValidator()]\n",
    "label_validators = [HFTokenAlignmentValidator(tokenizer=tokenizer.tokenizer)]\n",
    "formatters = HFDatasetFormatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803c8d5",
   "metadata": {},
   "source": [
    "## üîÑ Step 4: Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1393f674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (550 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "processed_data = []\n",
    "\n",
    "# üîÅ Pipeline loop\n",
    "for idx, example in enumerate(docs):\n",
    "    text, entities = parser.parse(example)\n",
    "    \n",
    "    for validator in parse_validators:\n",
    "        validator.validate(text, entities)\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_entity_spans = aligner.align(entities, tokens)\n",
    "\n",
    "    labels = labeler.label(tokens, token_entity_spans)\n",
    "    encoded_labels = label_encoder.encode(labels)\n",
    "\n",
    "    for validator in label_validators:\n",
    "        validator.validate(tokens, token_entity_spans)\n",
    "\n",
    "    record = ChiselRecord(\n",
    "                id=str(idx),\n",
    "                chunk_id=0,\n",
    "                text=tokenizer.tokenizer.decode([token.id for token in tokens]),\n",
    "                tokens=tokens,\n",
    "                input_ids=[token.id for token in tokens],\n",
    "                attention_mask=[1] * len(tokens),\n",
    "                entities=[tes.entity for tes in token_entity_spans],\n",
    "                bio_labels=labels,\n",
    "                labels=encoded_labels\n",
    "            )\n",
    "    processed_data.append(record)\n",
    "\n",
    "data = formatters.format(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004cc7b6",
   "metadata": {},
   "source": [
    "### ‚úÖ Output\n",
    "You now have a model ready hugginface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3000c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'chunk_id', 'tokens', 'input_ids', 'attention_mask', 'labels', 'bio_labels'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
