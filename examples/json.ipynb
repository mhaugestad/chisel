{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e944ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/workspaces/chisel/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45cfd0a",
   "metadata": {},
   "source": [
    "# ðŸ‘— Example: Processing Fashion Brand NER (JSON Format) with Chisel\n",
    "\n",
    "This example shows how to preprocess the explosion/ner-fashion-brands dataset into ChiselRecord objects for training transformer-based NER models using BILO labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f7bcd1",
   "metadata": {},
   "source": [
    "## ðŸ“¥ Step 1: Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c7d8fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"explosion/ner-fashion-brands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a229a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's all preference for which looks better, personally I feel that the more natural the hair looks the better the style, which for me means going with a matte finish which leaves the hair looking as natural as possible while still holding it in place\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449b2c09",
   "metadata": {},
   "source": [
    "## ðŸ§© Step 2: Implement a JSON Span Parser\n",
    "The dataset provides character-level spans in a spans field. We write a parser that extracts these into Chisel's EntitySpan format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48f17605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from chisel.extraction.base.protocols import Parser\n",
    "from chisel.extraction.models.models import EntitySpan\n",
    "\n",
    "class JSONSpanParser(Parser):\n",
    "    def parse(self, doc: dict) -> Tuple[str, List[EntitySpan]]:\n",
    "        text = doc[\"text\"]\n",
    "        entities = [\n",
    "            EntitySpan(\n",
    "                text=text[e[\"start\"]:e[\"end\"]],\n",
    "                start=e[\"start\"],\n",
    "                end=e[\"end\"],\n",
    "                label=e[\"label\"]\n",
    "            )\n",
    "            for e in doc.get(\"spans\", [])\n",
    "        ]\n",
    "        return text, entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eea4b3",
   "metadata": {},
   "source": [
    "## ðŸ”§ Step 3: Initialize Chisel Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "717023e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chisel.extraction.tokenizers.hf_tokenizer import HFTokenizer\n",
    "from chisel.extraction.span_aligners.token_span_aligner import TokenSpanAligner\n",
    "from chisel.extraction.labelers.bilo_labeler import BILOLabeler\n",
    "from chisel.extraction.labelers.label_encoder import SimpleLabelEncoder\n",
    "from chisel.extraction.validators.validators import DefaultParseValidator, HFTokenAlignmentValidator\n",
    "from chisel.extraction.formatters.torch_formatter import TorchDatasetFormatter\n",
    "from chisel.extraction.models.models import ChiselRecord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b40a9a",
   "metadata": {},
   "source": [
    "## Component Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5f1ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JSONSpanParser()\n",
    "tokenizer = HFTokenizer(model_name=\"bert-base-cased\")\n",
    "aligner = TokenSpanAligner()\n",
    "labeler = BILOLabeler()\n",
    "\n",
    "label_encoder = SimpleLabelEncoder(label_to_id={\n",
    "    'O': 0,\n",
    "    'B-FASHION_BRAND': 1,\n",
    "    'I-FASHION_BRAND': 2,\n",
    "    'L-FASHION_BRAND': 3,\n",
    "    'U-FASHION_BRAND': 4,\n",
    "})\n",
    "\n",
    "parse_validators = [DefaultParseValidator(on_error=\"raise\")]\n",
    "label_validators = [HFTokenAlignmentValidator(tokenizer=tokenizer.tokenizer, on_error=\"raise\")]\n",
    "formatter = TorchDatasetFormatter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e801e30",
   "metadata": {},
   "source": [
    "## ðŸ”„ Step 4: Run the Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0be6b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (525 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "processed_data = []\n",
    "\n",
    "for idx, example in enumerate(ds[\"train\"]):\n",
    "    text, entities = parser.parse(example)\n",
    "\n",
    "    # ðŸ§ª Per-span validation â€” skip bad spans\n",
    "    valid_spans = []\n",
    "    for span in entities:\n",
    "        try:\n",
    "            for validator in parse_validators:\n",
    "                validator.validate(text, span)\n",
    "            valid_spans.append(span)\n",
    "        except ValueError:\n",
    "            continue \n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_entity_spans = aligner.align(entities, tokens)\n",
    "\n",
    "    labels = labeler.label(tokens, token_entity_spans)\n",
    "    encoded_labels = label_encoder.encode(labels)\n",
    "\n",
    "    # ðŸ§ª Per-span validation â€” skip bad spans\n",
    "    valid_token_spans = []\n",
    "    for span in token_entity_spans:\n",
    "        try:\n",
    "            for validator in label_validators:\n",
    "                validator.validate(tokens, span)\n",
    "            valid_token_spans.append(span)\n",
    "        except ValueError:\n",
    "            continue  # Optionally log or collect stats on dropped spans\n",
    "\n",
    "    record = ChiselRecord(\n",
    "        id=str(idx),\n",
    "        chunk_id=0,\n",
    "        text=tokenizer.tokenizer.decode([token.id for token in tokens]),\n",
    "        tokens=tokens,\n",
    "        input_ids=[token.id for token in tokens],\n",
    "        attention_mask=[1] * len(tokens),\n",
    "        entities=[tes.entity for tes in valid_token_spans],\n",
    "        bio_labels=labels,\n",
    "        labels=encoded_labels\n",
    "    )\n",
    "    processed_data.append(record)\n",
    "\n",
    "data = formatter.format(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3727a8",
   "metadata": {},
   "source": [
    "### âœ… Output\n",
    "You now have a torch dataset ready for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd1d4338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 1135,   112,   188,  1155, 12629,  1111,  1134,  2736,  1618,   117,\n",
       "          7572,   146,  1631,  1115,  1103,  1167,  2379,  1103,  1716,  2736,\n",
       "          1103,  1618,  1103,  1947,   117,  1134,  1111,  1143,  2086,  1280,\n",
       "          1114,   170, 22591,  1566,  3146,  1134,  2972,  1103,  1716,  1702,\n",
       "          1112,  2379,  1112,  1936,  1229,  1253,  2355,  1122,  1107,  1282]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1]),\n",
       " 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c697470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23669): 1, 0\n",
      "(2298): 1, 0\n",
      "(1303): 1, 0\n",
      "(117): 1, 0\n",
      "(3983): 1, 0\n",
      "(787): 1, 0\n",
      "(189): 1, 0\n",
      "(1899): 1, 0\n",
      "(1330): 1, 0\n",
      "(1141): 1, 0\n",
      "(2589): 1, 0\n",
      "(1111): 1, 0\n",
      "(1103): 1, 0\n",
      "(3813): 1, 0\n",
      "(1107): 1, 0\n",
      "(1103): 1, 0\n",
      "(27103): 1, 1\n",
      "(2101): 1, 3\n",
      "(2984): 1, 0\n",
      "(119): 1, 0\n"
     ]
    }
   ],
   "source": [
    "for idx, mask, label in zip(\n",
    "    data[1][\"input_ids\"][0:20], \n",
    "    data[1][\"attention_mask\"][0:20], \n",
    "    data[1][\"labels\"][0:20]\n",
    "):\n",
    "    print(f\"({idx}): {mask}, {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
